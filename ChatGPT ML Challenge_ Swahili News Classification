{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bAy2bezKJhMD"},"outputs":[],"source":["pip install lightgbm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEihheiWJ9Jf"},"outputs":[],"source":["pip install imblearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rraZ3XpJ-H6"},"outputs":[],"source":["pip install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvcLl27kJ-lF"},"outputs":[],"source":["pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFLJQf2PJ-vi"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import re\n","import lightgbm as lgb\n","import optuna\n","from sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n","from sklearn.preprocessing import PolynomialFeatures\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import roc_auc_score\n","\n","from xgboost.callback import EarlyStopping\n","from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from catboost import CatBoostClassifier\n","import gc\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n","import xgboost as xgb\n","from itertools import combinations\n","from copy import deepcopy\n","from functools import partial\n","from optuna.integration import OptunaSearchCV\n","from optuna.samplers import RandomSampler\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.compose import make_column_selector\n","from geopy.distance import geodesic\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4r86C2blJ-6-"},"outputs":[],"source":["train_df = pd.read_csv(\"/content/Train (16).csv\")\n","test_df = pd.read_csv(\"/content/Test (16).csv\")\n","#test_df = test_df.drop(\"swahili_id\",axis=1)"]},{"cell_type":"code","source":[],"metadata":{"id":"B3YqXPUdo1p1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgaFlykwNLZz"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mljb2pcPhZ1k"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6mqqj7B5Kdy"},"outputs":[],"source":["test_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNXXbFG0dNMp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCVTbV4_5Ktd"},"outputs":[],"source":["train_df[\"category\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVmBq-pmNLnB"},"outputs":[],"source":["mapping_dict = {'Kitaifa': 0, 'Biashara': 1, 'michezo': 2,\"Kimataifa\": 3,\"Burudani\": 4,}\n","target = train_df.category.map(mapping_dict)\n","train_df = train_df.drop([\"id\",\"category\"],1)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"GUdqFqFtZ-Ij"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c3H5EaY7d-MR"},"outputs":[],"source":["import string\n","\n","def derive_new_features(data):\n","    data = data[['content']].copy()\n","\n","    #Unique words Count\n","    data['unique_words_count'] = data.content.apply(lambda x: len(set(str(x).split())))\n","\n","    #Punctuation count\n","    data['punctuation_count'] = data.content.apply(lambda x: len([x for x in x.lower().split() if x in string.punctuation]))\n","\n","    #Upper case words count\n","    data['uppercase_words_count'] = data.content.apply(lambda x: sum([x.isupper() for x in x.split()]))\n","\n","    #Title words count\n","    data['title_words_count'] = data.content.apply(lambda x: sum([x.istitle() for x in x.split()]))\n","\n","    return data.drop(['content'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wT7ARAHZd-Xq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"360UvAjXGjg8"},"outputs":[],"source":["from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rhvPRzL6GwJj"},"outputs":[],"source":["eng_stopwords = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SOUJAf01HJ62"},"outputs":[],"source":["import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FC9w8HCND7I4"},"outputs":[],"source":["## Number of words in the text ##\n","train_df['num_words'] = train_df['content'].apply(lambda x: len(str(x).split()))\n","test_df['num_words'] = test_df['content'].apply(lambda x: len(str(x).split()))\n","\n","## Number of unique words in the text ##\n","train_df['num_unique_words'] = train_df['content'].apply(lambda x: len(set(str(x).split())))\n","test_df['num_unique_words'] = test_df['content'].apply(lambda x: len(set(str(x).split())))\n","\n","## Number of characters in the text ##\n","train_df['num_chars'] = train_df['content'].apply(lambda x: len(str(x)))\n","test_df['num_chars'] = test_df['content'].apply(lambda x: len(str(x)))\n","\n","## Number of soptwords in the text ##\n","train_df['num_stopwrods'] = train_df['content'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n","test_df['num_stopwrods'] = test_df['content'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n","\n","## Number of punctuations in the text ##\n","train_df['num_punctuations'] = train_df['content'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","test_df['num_punctuations'] = test_df['content'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","\n","## Number of title case words in the text ##\n","train_df['num_words_upper'] = train_df['content'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","test_df['num_words_upper'] = test_df['content'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","\n","## Average length of the words in the text ##\n","train_df['mean_word_len'] = train_df['content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","test_df['mean_word_len'] = test_df['content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLEnKMQeECXd"},"outputs":[],"source":["train_df['num_words'] = train_df['content'].apply(lambda x: len(str(x).split()))\n","test_df['num_words'] = test_df['content'].apply(lambda x: len(str(x).split()))\n","train_df['mean_word_len'] = train_df['content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","test_df['mean_word_len'] = test_df['content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mKNMLnLmHO9I"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbcxEoRooN89"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","import re\n","from collections import Counter\n","import nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuquSiLApNXQ"},"outputs":[],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZAAGXQpm26_"},"outputs":[],"source":["stemmer = nltk.PorterStemmer()\n","\n","class TextToWordCounterTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, remove_punctuation=True, lower_case=True, stemming=True, replace_numbers=True, remove_stopwords=True):\n","        self.remove_punctuation = remove_punctuation\n","        self.lower_case = lower_case\n","        self.stemming = stemming\n","        self.replace_numbers = replace_numbers\n","        self.remove_stopwords = remove_stopwords\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        word_counters = []\n","        for text in X['content']:\n","            if self.lower_case:\n","                text = text.lower()\n","            if self.replace_numbers:\n","                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text) #replace any numerical character with 'NUMBER'\n","            if self.remove_punctuation:\n","                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n","            if self.remove_stopwords:\n","                words = [word for word in text.split() if word not in ENGLISH_STOP_WORDS]\n","                text = ' '.join(words)\n","            word_list = nltk.word_tokenize(text)\n","            word_count = Counter(word_list)\n","            if self.stemming:\n","                stemmed_word_count = Counter()\n","                for word in word_list:\n","                    stemmed_word = stemmer.stem(word)\n","                    stemmed_word_count[stemmed_word] += 1\n","                word_count = stemmed_word_count\n","            word_counters.append(word_count)\n","        return np.array(word_counters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVYfbFXHm6eE"},"outputs":[],"source":["X_word_count = TextToWordCounterTransformer().fit_transform(train_df)\n","X_word_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLI-Crb-m-sb"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","\n","class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, vocabulary_size=1000):\n","        self.vocabulary_size = vocabulary_size\n","    def fit(self, X, y=None):\n","        total_count = Counter()\n","        for word_count in X:\n","            for word, count in word_count.items():\n","                total_count[word] += min(count, 10)\n","        most_common = total_count.most_common()[:self.vocabulary_size]\n","        self.most_common_ = most_common\n","        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}  #spare an index for excluded words\n","        return self\n","    def transform(self, X, y=None):\n","        data = []\n","        rows = []\n","        cols = []\n","        for row, word_count in enumerate(X):\n","            for word, count in word_count.items():\n","                data.append(count)\n","                rows.append(row)\n","                cols.append(self.vocabulary_.get(word, 0))\n","        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdoSxygEnBsP"},"outputs":[],"source":["vectorizer = WordCounterToVectorTransformer(vocabulary_size=1000)\n","vectorizer.fit_transform(X_word_count).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcc18hhNnEUZ"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","\n","preprocess_pipeline = Pipeline([\n","    ('text_to_word_count', TextToWordCounterTransformer()),\n","    ('word_count_to_vector', WordCounterToVectorTransformer(vocabulary_size=14000)),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5Tbp5NunG5w"},"outputs":[],"source":["class NewFeaturesAdderTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        return np.array(derive_new_features(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z84asb-ZnJXF"},"outputs":[],"source":["NewFeaturesAdderTransformer().fit_transform(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUIrBf8wqAAX"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","label_encoder = LabelEncoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guVk54arnMMk"},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","\n","full_pipeline = ColumnTransformer([\n","    ('feature_adder', NewFeaturesAdderTransformer(), ['text']),\n","    ('text_pipeline', preprocess_pipeline, ['text']),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gk8-C7rPpwUU"},"outputs":[],"source":["from sklearn.metrics import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNuaqPK8nR0f"},"outputs":[],"source":["X_train_transformed = full_pipeline.fit_transform(train_df)\n","y_train_transformed = label_encoder.fit_transform(target.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9H3vC2aoHk3l"},"outputs":[],"source":["target.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIddrWI2qIzV"},"outputs":[],"source":["from sklearn.metrics import log_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Z867dY5gVnr"},"outputs":[],"source":["from sklearn.preprocessing import label_binarize\n","from sklearn.utils import class_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLBuP1hnHuxv"},"outputs":[],"source":["from sklearn import model_selection,metrics, naive_bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okQf-IcrIj_S"},"outputs":[],"source":["train_X = train_df\n","test_X = test_df\n","train_y =target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibcWpnFUhl6j"},"outputs":[],"source":["train_X = train_X.fillna(train_X.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MEA1TK6r4ti"},"outputs":[],"source":["scaler = MinMaxScaler()\n","train_X = scaler.fit_transform(train_X)\n","test_X = scaler.transform(test_X)\n","\n","train_X = pd.DataFrame(train_X)\n","test_X = pd.DataFrame(test_X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HOSWNQ5hPy9"},"outputs":[],"source":["train_X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ePequQrVIwL"},"outputs":[],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f930HKmU6WF"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n","\n","# Define a custom dataset for DataLoader\n","class TextClassificationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = str(self.texts.iloc[index]['content'])\n","        label = self.labels.iloc[index]\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","            truncation=True,\n","        )\n","        return {\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(label, dtype=torch.long),\n","        }\n","\n","# Initialize the BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n","\n","# Define training and validation datasets\n","train_dataset = TextClassificationDataset(train_X, train_y, tokenizer, max_length=128)\n","val_dataset = TextClassificationDataset(val_X, val_y, tokenizer, max_length=128)\n","\n","# Create DataLoader for efficient batch processing\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Define optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","total_steps = len(train_dataloader) * 10  # 10 epochs\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=total_steps, div_factor=10, final_div_factor=50)\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(10):  # 10 epochs for demonstration\n","    model.train()\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    total_val_samples = 0\n","\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","            val_loss += loss.item()\n","            predictions = logits.argmax(dim=1)\n","            val_correct += (predictions == batch[\"labels\"]).sum().item()\n","            total_val_samples += batch[\"labels\"].size(0)\n","\n","    val_accuracy = val_correct / total_val_samples\n","    print(f\"Epoch {epoch + 1}: Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuQ6l9m2U6gF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiy1nbdwU6rA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dT2jal4mU61F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOGF1BW9U69S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2I9KR8oRU7Fo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQ5S3XmRU7Nq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Rk3gGw4U7Wl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKrHohtRU7ek"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tg8OEwCP63mU"},"outputs":[],"source":["import optuna\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.pipeline import Pipeline\n","\n","def objective(trial):\n","    param = {}\n","    param['objective'] = 'multi:softprob'\n","    param['eta'] = trial.suggest_float('eta', 0.01, 0.1, log=True)\n","    param['max_depth'] = trial.suggest_int('max_depth', 3, 10)\n","    param['min_child_weight'] = trial.suggest_float('min_child_weight', 0.1, 10.0, log=True)\n","    param['subsample'] = trial.suggest_float('subsample', 0.1, 1.0)\n","    param['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.1, 1.0)\n","    param['seed'] = 0\n","    param['num_class'] = 5\n","\n","    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n","\n","    # Use TF-IDF vectorization for text data\n","    tfidf_vectorizer = TfidfVectorizer()\n","    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['content'])\n","    X_val_tfidf = tfidf_vectorizer.transform(X_val['content'])\n","\n","    xgtrain = xgb.DMatrix(X_train_tfidf, label=y_train, enable_categorical=True)\n","    xgval = xgb.DMatrix(X_val_tfidf, label=y_val, enable_categorical=True)\n","\n","    evals_result = {}\n","    text_clf = Pipeline([\n","        ('clf', xgb.XGBClassifier(**param, eval_metric='mlogloss', early_stopping_rounds=50, verbose=False)),\n","    ])\n","\n","    # Provide the validation set explicitly for early stopping\n","    model = text_clf.fit(X_train_tfidf, y_train, clf__eval_set=[(X_val_tfidf, y_val)])\n","\n","    return model['clf'].best_score\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=100)\n","\n","# Check if any trials have been completed\n","if len(study.trials) == 0:\n","    print(\"No trials have been completed.\")\n","    exit()\n","\n","# Get the best trial and its parameters\n","best_trial = study.best_trial\n","best_params = best_trial.params\n","print(\"Best params:\", best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXpukmUvOiYZ"},"outputs":[],"source":["k_folds = 5\n","kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","# Initialize an array to store validation scores for each fold\n","validation_scores = []\n","\n","# Perform k-fold cross-validation\n","for train_index, val_index in kf.split(train_X, train_y):\n","    X_train_fold, X_val_fold = train_X.iloc[train_index], train_X.iloc[val_index]\n","    y_train_fold, y_val_fold = train_y.iloc[train_index], train_y.iloc[val_index]\n","\n","    # Use TF-IDF vectorization for text data\n","    tfidf_vectorizer = TfidfVectorizer()\n","    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_fold['content'])\n","    X_val_tfidf = tfidf_vectorizer.transform(X_val_fold['content'])\n","\n","    # Convert data to DMatrix format\n","    xgtrain = xgb.DMatrix(X_train_tfidf, label=y_train_fold, enable_categorical=True)\n","    xgval = xgb.DMatrix(X_val_tfidf, label=y_val_fold, enable_categorical=True)\n","\n","    # Create the XGBoost model with the best parameters\n","    model = xgb.XGBClassifier(\n","        objective='multi:softprob',\n","        eta=best_params['eta'],\n","        max_depth=best_params['max_depth'],\n","        min_child_weight=best_params['min_child_weight'],\n","        subsample=best_params['subsample'],\n","        colsample_bytree=best_params['colsample_bytree'],\n","        seed=0,\n","        num_class=5,\n","        eval_metric='mlogloss',\n","        early_stopping_rounds=50,\n","        verbose=False\n","    )\n","\n","    # Fit the model\n","    model.fit(X_train_tfidf, y_train_fold, eval_set=[(X_val_tfidf, y_val_fold)])\n","\n","    # Evaluate the model on the validation set\n","    validation_score = model.best_score\n","    validation_scores.append(validation_score)\n","\n","# Calculate the average validation score across all folds\n","avg_validation_score = np.mean(validation_scores)\n","print(\"Average validation score:\", avg_validation_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oemcaOIhPvUD"},"outputs":[],"source":["X_test_tfidf = tfidf_vectorizer.transform(test_X['content'])\n","#xgtest = xgb.DMatrix(X_test_tfidf)\n","pred_test_y = model.predict(X_test_tfidf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cpJuKTl_yuQ"},"outputs":[],"source":["def runXGB(train_X, train_y, params, num_rounds=200):\n","    # Use the same TF-IDF vectorizer for both training and validation data\n","    tfidf_vectorizer = TfidfVectorizer()\n","    X_train_tfidf = tfidf_vectorizer.fit_transform(train_X['content'])\n","    xgtrain = xgb.DMatrix(X_train_tfidf, label=train_y, enable_categorical=True)\n","\n","    # Train the model\n","    model = xgb.train(params, xgtrain, num_boost_round=num_rounds)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h-Yx4jMFaEe"},"outputs":[],"source":["num_classes = 5\n","\n","# Update the number of rounds if needed\n","best_params['num_class'] = num_classes\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n","cv_scores = []\n","pred_full_test = 0\n","pred_train = np.empty((0, num_classes))  # Initialize an empty array with the desired shape\n","\n","for dev_index, val_index in kf.split(train_X):\n","    dev_X, val_X = train_X.iloc[dev_index], train_X.iloc[val_index]\n","    dev_y, val_y = train_y[dev_index], train_y[val_index]\n","    model = runXGB(train_X=dev_X, train_y=dev_y, params=best_params)\n","\n","    tfidf_vectorizer = TfidfVectorizer()\n","    X_val_tfidf = tfidf_vectorizer.fit_transform(val_X['content'])\n","    xgval = xgb.DMatrix(X_val_tfidf)\n","\n","    pred_val_y = model.predict(xgval)\n","\n","    # Remove any NaN values from pred_val_y\n","    pred_val_y = pred_val_y[~np.isnan(pred_val_y)]\n","\n","    # Reshape pred_val_y to ensure it has two dimensions\n","    pred_val_y = pred_val_y.reshape(-1, 1)\n","\n","    # Create an intermediate array with the correct shape\n","    intermediate_array = np.empty((pred_val_y.shape[0], num_classes))\n","    intermediate_array[:] = np.nan\n","\n","    # Copy the non-NaN values to the intermediate array\n","    intermediate_array[:, :pred_val_y.shape[1]] = pred_val_y\n","\n","    # Append the intermediate array to pred_train\n","    pred_train = np.append(pred_train, intermediate_array, axis=0)\n","\n","    cv_scores.append(metrics.log_loss(val_y, intermediate_array, labels=list(range(num_classes))))\n","\n","    X_test_tfidf = tfidf_vectorizer.transform(test_X['content'])\n","    xgtest = xgb.DMatrix(X_test_tfidf)\n","    pred_test_y = model.predict(xgtest)\n","    pred_full_test += pred_test_y\n","\n","# Calculate the mean predictions for all folds\n","pred_train = np.nanmean(pred_train, axis=0)\n","\n","print('cv scores:', cv_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3vYNhsLFpJR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Pmr_3UvFaQq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2OjYOVdCv1j"},"outputs":[],"source":["def objective(trial):\n","    params = {\n","        'objective': 'multiclass',\n","        'num_class': 5,  # Number of classes\n","        'boosting_type': 'gbdt',\n","        'metric': 'multi_logloss',\n","        'verbosity': -1,\n","        'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n","        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n","        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n","        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n","        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n","    }\n","\n","    # Perform K-fold cross-validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    cv_scores = []\n","    for train_index, val_index in kf.split(train_X):\n","        train_X_fold, val_X_fold = train_X.iloc[train_index], train_X.iloc[val_index]\n","        train_y_fold, val_y_fold = train_y.iloc[train_index], train_y.iloc[val_index]\n","\n","        dtrain = lgb.Dataset(train_X_fold, label=train_y_fold)\n","        dval = lgb.Dataset(val_X_fold, label=val_y_fold)\n","\n","        model = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dval],\n","                          early_stopping_rounds=50, verbose_eval=False)\n","\n","        val_pred = model.predict(val_X_fold)\n","        cv_score = log_loss(val_y_fold, val_pred)\n","        cv_scores.append(cv_score)\n","\n","    return np.mean(cv_scores)\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=100)\n","\n","best_params = study.best_params\n","final_model = lgb.train(best_params, lgb.Dataset(train_X, label=train_y), num_boost_round=1000)\n","\n","pred_test = final_model.predict(test_X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhKdSOoARqnx"},"outputs":[],"source":["pred_test.shape\n","#test_X.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xe_HLu6sHZkU"},"outputs":[],"source":["test = pd.read_csv(\"/content/Test (16).csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pdzal5PMH-kf"},"outputs":[],"source":["pred_test_proba = pred_test_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avi_Xp1FHD4w"},"outputs":[],"source":["submission = []\n","for i in range(len(test_X)):\n","    row = {\n","        \"swahili_id\": test['swahili_id'][i],\n","    }\n","\n","    if pred_test_proba.ndim == 1:  # Binary classification case\n","        row[\"kitaifa\"] = pred_test_proba[i]\n","        row[\"michezo\"] = 1 - pred_test_proba[i]\n","        row[\"biashara\"] = 0\n","        row[\"kimataifa\"] = 0\n","        row[\"burudani\"] = 0\n","    else:  # Multiclass classification case\n","        row[\"kitaifa\"] = pred_test_proba[i, 0]\n","        row[\"michezo\"] = pred_test_proba[i, 1]\n","        row[\"biashara\"] = pred_test_proba[i, 2]\n","        row[\"kimataifa\"] = pred_test_proba[i, 3]\n","        row[\"burudani\"] = pred_test_proba[i, 4]\n","\n","    submission.append(row)\n","\n","# Convert submission to a DataFrame and save it to a file\n","submission_df = pd.DataFrame(submission)\n","submission_df.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZooir8dQxUk"},"outputs":[],"source":["test = pd.read_csv(\"/content/Test (16).csv\")\n","\n","submission_df = pd.DataFrame(columns=[\"swahili_id\", \"kitaifa\", \"michezo\", \"biashara\", \"kimataifa\", \"burudani\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_f7KsPWSxex"},"outputs":[],"source":["for i in range(len(test['swahili_id'])):\n","    row = {\n","        \"swahili_id\": test['swahili_id'][i],  # Replace `swahili_ids` with your list/array of swahili_id values\n","        \"kitaifa\": pred_test_y[i][0],  # Replace `predictions` with your list/array of predictions\n","        \"michezo\": pred_test_y[i][1],\n","        \"biashara\": pred_test_y[i][2],\n","        \"kimataifa\": pred_test_y[i][3],\n","        \"burudani\": pred_test_y[i][4]\n","    }\n","    submission_df = submission_df.append(row, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_G4IXCXtTjZl"},"outputs":[],"source":["submission_df.set_index(\"swahili_id\", inplace=True)\n","submission_df.to_csv(\"submission swahili 4.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8y14oRCTjmG"},"outputs":[],"source":["submission_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZ99u8RWBZs7"},"outputs":[],"source":["encoding_dim = 50\n","input_layer = Input(shape=(train.shape[1],))\n","\n","\n","encoded = Dense(26, activation='relu')(input_layer)\n","encoded = Dense(encoding_dim, activation='relu')(encoded)\n","\n","decoded = Dense(26, activation='relu')(encoded)\n","decoded = Dense(train.shape[1], activation='linear')(decoded)\n","\n","autoencoder = Model(input_layer, decoded)\n","\n","\n","autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","autoencoder.fit(train, target, epochs=5, batch_size=16, shuffle=True)\n","\n","encoder = Model(input_layer, encoded)\n","encoded_data = encoder.predict(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWr7fhZgC3PE"},"outputs":[],"source":["test = pd.DataFrame(encoder.predict(test))\n","train = pd.DataFrame(encoded_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdh_E2GPUtst"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(train, target, test_size=0.1, random_state=43)\n","\n","\n","oversampler = RandomOverSampler(random_state=42)\n","smote_sampler = SMOTE(random_state=42)\n","adasyn_sampler = ADASYN(random_state=42)\n","under_sampler = RandomUnderSampler(random_state=42)\n","\n","x_train_resampled, y_train_resampled = under_sampler.fit_resample(x_train, y_train)\n","x_train_resampled, y_train_resampled = oversampler.fit_resample(x_train, y_train)\n","x_train_resampled, y_train_resampled = smote_sampler.fit_resample(x_train_resampled, y_train_resampled)\n","X_train, y_train = adasyn_sampler.fit_resample(x_train_resampled, y_train_resampled)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcZiP-hdeE7L"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmLt5byodjI_"},"outputs":[],"source":["class Splitter:\n","    def __init__(self, n_splits=5, cat_df=pd.DataFrame(), test_size=0.1):\n","        self.n_splits = n_splits\n","        self.cat_df = cat_df\n","        self.test_size = test_size\n","\n","    def split_data(self, X, y, random_state_list):\n","        for random_state in random_state_list:\n","            kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n","            for train_index, val_index in kf.split(X, y):\n","                X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","                y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","                yield X_train, X_val, y_train, y_val, val_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVmVYAs0SJaN"},"outputs":[],"source":["len(y_train.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdByMUKCd5D0"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","class Classifier:\n","    def __init__(self, n_estimators=100, device=\"cpu\", random_state=42):\n","        self.n_estimators = n_estimators\n","        self.device = device\n","        self.random_state = random_state\n","        self.models = self.get_models()\n","        self.models_name = list(self.get_models().keys())\n","        self.len_models = len(self.models)\n","\n","    param_distributions = {\n","    'learning_rate': [0.1, 0.01, 0.001],\n","    'max_depth': [3, 5, 7]}\n","\n","    def get_models(self):\n","        xgb_optuna1 = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.0799142092833258,\n","            'booster': 'gbtree',\n","            'lambda': 0.09087023987059259,\n","            'alpha': 0.09411170677435933,\n","            'subsample': 0.9520470731432641,\n","            'colsample_bytree': 0.8681495841058746,\n","            'max_depth': 10,\n","            'min_child_weight': 1,\n","            'eta': 0.40270631476222696,\n","            'gamma': 0.11026102855113062,\n","            'n_jobs': 1,\n","            'objective': 'multi:softmax',\n","            'num_class': 5,\n","            'verbosity': 0,\n","            'random_state': self.random_state,\n","        }\n","\n","        # Update the remaining XGBoost models with 'objective': 'multi:softmax' and 'num_class': num_classes\n","\n","        lgb_optuna1 = {\n","            'num_iterations': 100,\n","            'lambda': 9.168936765382027,\n","            'alpha': 7.715088592076653,\n","            'min_child_weight': 1,\n","            'lambda_l1': 5.761602713004586e-05,\n","            'lambda_l2': 0.024835256743135217,\n","            'num_leaves': 140,\n","            'feature_fraction': 0.45559944978167594,\n","            'bagging_fraction': 0.9626014364266151,\n","            'bagging_freq': 1,\n","            'min_child_samples': 55,\n","            'learning_rate': 0.24422408954529448,\n","            'max_depth': 9,\n","            'min_data_in_leaf': 18,\n","            'subsample': 0.8929579782170575,\n","            'colsample_bytree': 0.7493251602963501,\n","            'subsample_freq': 248,\n","            'device': self.device,\n","            'random_state': self.random_state,\n","            'is_unbalance': True\n","        }\n","\n","        # Update the remaining LightGBM models with 'is_unbalance': True\n","\n","        cat_optuna1 = {\n","            'learning_rate': 0.08223626901086092,\n","            'depth': 10,\n","            'l2_leaf_reg': 5.246221902059309,\n","            'bagging_temperature': 4.117053990317477,\n","            'border_count': 112,\n","            'iterations': 536,\n","            'random_strength': 6.235621173426315,\n","            'grow_policy': 'Depthwise',\n","            'scale_pos_weight': 0.4923863689375505,\n","            'colsample_bylevel': 0.770515289996092,\n","            'task_type': self.device.upper(),\n","            'verbose': False,\n","            'allow_writing_files': False,\n","            'random_state': self.random_state,\n","            'loss_function': 'MultiClass',\n","            'num_class': 5\n","        }\n","\n","        # Update the remaining CatBoost models with 'loss_function': 'MultiClass' and 'num_class': num_classes\n","\n","        random_optuna = {\n","            'n_estimators': 200,\n","            'max_depth': 10\n","        }\n","\n","        dicision1_optuna2 = {\n","            'criterion': 'entropy',\n","            'max_depth': 7,\n","            'min_samples_split': 12,\n","            'min_samples_leaf': 9,\n","            'max_features': 0.9587526399028644,\n","            'min_impurity_decrease': 0.001026301339281785,\n","            'min_weight_fraction_leaf': 0.0005424343967970673,\n","            'splitter': 'best'\n","        }\n","\n","        dicision2_optuna2 = {\n","            'criterion': 'gini',\n","            'max_depth': 12,\n","            'min_samples_split': 27,\n","            'min_samples_leaf': 10,\n","            'max_features': 0.8183576248616834,\n","            'min_impurity_decrease': 4.762568722942594e-06,\n","            'min_weight_fraction_leaf': 0.00038543223364360826,\n","            'splitter': 'best'\n","        }\n","\n","        # Update the remaining classifiers with appropriate parameters for multi-class classification (e.g., MultinomialNB, SVM, etc.)\n","\n","        models = {\n","            \"xgbo1\": OptunaSearchCV(xgb.XGBClassifier(**xgb_optuna1), n_trials=100),\n","            # Include the remaining XGBoost models\n","            \"lgbo1\": OptunaSearchCV(lgb.LGBMClassifier(**lgb_optuna1), n_trials=100),\n","            # Include the remaining LightGBM models\n","            \"cato1\": OptunaSearchCV(CatBoostClassifier(**cat_optuna1), n_trials=100),\n","            # Include the remaining CatBoost models\n","            'dicision1': OptunaSearchCV(DecisionTreeClassifier(**dicision1_optuna2), n_trials=100),\n","            'dicision2': OptunaSearchCV(DecisionTreeClassifier(**dicision2_optuna2), n_trials=100),\n","            'random': OptunaSearchCV(RandomForestClassifier(**random_optuna), n_trials=100),\n","            'ada': OptunaSearchCV(AdaBoostClassifier(), n_trials=100),\n","            'svm': OptunaSearchCV(SVC(), n_trials=100),\n","            'logistic': OptunaSearchCV(LogisticRegressionCV(), n_trials=100),\n","            'nb': OptunaSearchCV(MultinomialNB(), n_trials=100),\n","            'knn': OptunaSearchCV(KNeighborsClassifier(), n_trials=100)\n","        }\n","        return models\n","\n","    def fit(self, X_train, y_train):\n","        for model in self.models.values():\n","            model.fit(X_train, y_train)\n","\n","    def predict(self, X_test):\n","        predictions = {}\n","        for name, model in self.models.items():\n","            predictions[name] = model.predict(X_test)\n","        return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qv7E0jPWGOf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zq2hFW7GWGv7"},"outputs":[],"source":["import xgboost as xgb\n","import lightgbm as lgb\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from catboost import CatBoostClassifier\n","from optuna.integration import OptunaSearchCV\n","from scipy.stats import uniform, randint\n","\n","class Classifier:\n","    def __init__(self, n_estimators=100, device=\"cpu\", random_state=42):\n","        self.n_estimators = n_estimators\n","        self.device = device\n","        self.random_state = random_state\n","        self.models = self.get_models()\n","        self.models_name = list(self.get_models().keys())\n","        self.len_models = len(self.models)\n","\n","    def get_models(self):\n","        param_distributions = {\n","        'n_estimators': distributions.IntUniformDistribution(100, 1000),\n","        'learning_rate': distributions.UniformDistribution(0.001, 0.1),\n","        'max_depth': distributions.IntUniformDistribution(3, 10),\n","        # Define the search space for other hyperparameters\n","    }\n","        xgb_optuna1 = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.0799142092833258,\n","            'booster': 'gbtree',\n","            'lambda': 0.09087023987059259,\n","            'alpha': 0.09411170677435933,\n","            'subsample': 0.9520470731432641,\n","            'colsample_bytree': 0.8681495841058746,\n","            'max_depth': 10,\n","            'min_child_weight': 1,\n","            'eta': 0.40270631476222696,\n","            'gamma': 0.11026102855113062,\n","            'n_jobs': 1,\n","            'objective': 'multi:softmax',\n","            'num_class': 5,\n","            'verbosity': 0,\n","            'random_state': self.random_state,\n","        }\n","\n","        # Update the remaining XGBoost models with 'objective': 'multi:softmax' and 'num_class': num_classes\n","\n","        lgb_optuna1 = {\n","            'num_iterations': 100,\n","            'lambda': 9.168936765382027,\n","            'alpha': 7.715088592076653,\n","            'min_child_weight': 1,\n","            'lambda_l1': 5.761602713004586e-05,\n","            'lambda_l2': 0.024835256743135217,\n","            'num_leaves': 140,\n","            'feature_fraction': 0.45559944978167594,\n","            'bagging_fraction': 0.9626014364266151,\n","            'bagging_freq': 1,\n","            'min_child_samples': 55,\n","            'learning_rate': 0.24422408954529448,\n","            'max_depth': 9,\n","            'min_data_in_leaf': 18,\n","            'subsample': 0.8929579782170575,\n","            'colsample_bytree': 0.7493251602963501,\n","            'subsample_freq': 248,\n","            'device': self.device,\n","            'random_state': self.random_state,\n","            'is_unbalance': True\n","        }\n","\n","        # Update the remaining LightGBM models with 'is_unbalance': True\n","\n","        cat_optuna1 = {\n","            'learning_rate': 0.08223626901086092,\n","            'depth': 10,\n","            'l2_leaf_reg': 5.246221902059309,\n","            'bagging_temperature': 4.117053990317477,\n","            'border_count': 112,\n","            'iterations': 536,\n","            'random_strength': 6.235621173426315,\n","            'grow_policy': 'Depthwise',\n","            'scale_pos_weight': 0.4923863689375505,\n","            'colsample_bylevel': 0.770515289996092,\n","            'task_type': self.device.upper(),\n","            'verbose': False,\n","            'allow_writing_files': False,\n","            'random_state': self.random_state,\n","            'loss_function': 'MultiClass',\n","            'num_class': 5\n","        }\n","\n","        # Update the remaining CatBoost models with 'loss_function': 'MultiClass' and 'num_class': num_classes\n","\n","        random_optuna = {\n","            'n_estimators': 200,\n","            'max_depth': 10\n","        }\n","\n","        dicision1_optuna2 = {\n","            'criterion': 'entropy',\n","            'max_depth': 7,\n","            'min_samples_split': 12,\n","            'min_samples_leaf': 9,\n","            'max_features': 0.9587526399028644,\n","            'min_impurity_decrease': 0.001026301339281785,\n","            'min_weight_fraction_leaf': 0.0005424343967970673,\n","            'splitter': 'best'\n","        }\n","\n","        dicision2_optuna2 = {\n","            'criterion': 'gini',\n","            'max_depth': 12,\n","            'min_samples_split': 27,\n","            'min_samples_leaf': 10,\n","            'max_features': 0.8183576248616834,\n","            'min_impurity_decrease': 4.762568722942594e-06,\n","            'min_weight_fraction_leaf': 0.00038543223364360826,\n","            'splitter': 'best'\n","        }\n","\n","        models = {\n","            \"xgbo1\": OptunaSearchCV(xgb.XGBClassifier(objective='multi:softmax', num_class=5),\n","                                    param_distributions=param_distributions, n_trials=100),\n","            # Include the remaining XGBoost models with objective='multi:softmax' and num_class=num_classes\n","            \"lgbo1\": OptunaSearchCV(lgb.LGBMClassifier(objective='multiclass', num_class=5),\n","                                    param_distributions=param_distributions, n_trials=100),\n","            # Include the remaining LightGBM models with objective='multiclass' and num_class=num_classes\n","            \"cato1\": OptunaSearchCV(CatBoostClassifier(loss_function='MultiClass', classes_count=5),\n","                                    param_distributions=param_distributions, n_trials=100),\n","            # Include the remaining CatBoost models with loss_function='MultiClass' and classes_count=num_classes\n","            'dicision1': OptunaSearchCV(DecisionTreeClassifier(),\n","                                        param_distributions=param_distributions, n_trials=100),\n","            'dicision2': OptunaSearchCV(DecisionTreeClassifier(),\n","                                        param_distributions=param_distributions, n_trials=100),\n","            'random': OptunaSearchCV(RandomForestClassifier(),\n","                                     param_distributions=param_distributions, n_trials=100),\n","            'ada': OptunaSearchCV(AdaBoostClassifier(),\n","                                  param_distributions=param_distributions, n_trials=100),\n","            'svm': OptunaSearchCV(SVC(),\n","                                  param_distributions=param_distributions, n_trials=100),\n","            'logistic': OptunaSearchCV(LogisticRegressionCV(),\n","                                       param_distributions=param_distributions, n_trials=100),\n","            'nb': OptunaSearchCV(MultinomialNB(),\n","                                 param_distributions=param_distributions, n_trials=100),\n","            'knn': OptunaSearchCV(KNeighborsClassifier(),\n","                                  param_distributions=param_distributions, n_trials=100)\n","        }\n","        return models\n","\n","    def fit(self, X_train, y_train):\n","        for model in self.models.values():\n","            model.fit(X_train, y_train)\n","\n","    def predict(self, X_new):\n","        predictions = {}\n","        for model_name, model in self.models.items():\n","            model_predictions = model.predict(X_new)\n","            predictions[model_name] = model_predictions\n","        return predictions\n","\n","\n","# Example usage:\n","\n","# Assuming you have your training data X_train and y_train\n","\n","# Instantiate the Classifier object\n","classifier = Classifier(n_estimators=100, device=\"cpu\", random_state=42)\n","\n","# Fit the Classifier with training data\n","classifier.fit(X_train, y_train)\n","\n","# Assuming you have new data X_new\n","\n","# Make predictions on new data\n","predictions = classifier.predict(X_new)\n","\n","# Access the predictions for each model\n","for model_name, model_predictions in predictions.items():\n","    print(f\"Model: {model_name}\")\n","    print(\"Predictions:\", model_predictions)\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAo4CVDnWG_y"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHbaKvcTRBKK"},"outputs":[],"source":["\n","classifier = Classifier(n_estimators=100, device=\"cpu\", random_state=42)\n","classifier.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GefJW9oFT7p-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hw1FPUFQT74G"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQtxCd8OaK9s"},"outputs":[],"source":["class OptunaWeights:\n","    def __init__(self, random_state, n_trials=100):\n","        self.study = None\n","        self.weights = None\n","        self.random_state = random_state\n","        self.n_trials = n_trials\n","\n","    def _objective(self, trial, y_true, y_preds):\n","      num_classes = len(np.unique(y_true))\n","      num_samples = y_true.shape[0]  # Number of samples in y_true\n","\n","      # Define the weights for the predictions from each model for each class\n","      weights = []\n","      for c in range(num_classes):\n","          weights_class = [\n","              trial.suggest_float(f\"weight_{c}_{n}\", 1e-15, 1)\n","              for n in range(len(y_preds))\n","          ]\n","          weights.append(weights_class)\n","\n","      # Calculate the weighted prediction for each class\n","      weighted_preds = []\n","      for c in range(num_classes):\n","          weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights[c])\n","          weighted_preds.append(weighted_pred)\n","\n","      # Combine the weighted predictions for all classes\n","      weighted_pred_all = np.vstack(weighted_preds).T\n","      weighted_pred_all = weighted_pred_all[:num_samples]  # Trim to match y_true\n","      score = accuracy_score(y_true, weighted_pred_all)\n","      return score\n","\n","    def fit(self, y_true, y_preds):\n","        optuna.logging.set_verbosity(optuna.logging.ERROR)\n","        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n","        pruner = optuna.pruners.HyperbandPruner()\n","        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n","        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n","        self.study.optimize(objective_partial, n_trials=self.n_trials)\n","\n","        num_classes = len(np.unique(y_true))\n","        self.weights = []\n","        for c in range(num_classes):\n","            weights_class = [self.study.best_params[f\"weight_{c}_{n}\"] for n in range(len(y_preds))]\n","            self.weights.append(weights_class)\n","\n","    def predict(self, y_preds):\n","        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n","        num_classes = len(self.weights)\n","        weighted_preds = []\n","        for c in range(num_classes):\n","            weights_class = np.array(self.weights[c]).reshape(1, -1)\n","            weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights_class)\n","            weighted_preds.append(weighted_pred)\n","        weighted_pred_all = np.vstack(weighted_preds).T\n","        return weighted_pred_all\n","\n","    def fit_predict(self, y_true, y_preds):\n","        self.fit(y_true, y_preds)\n","        return self.predict(y_preds)\n","\n","    def get_weights(self):\n","        return self.weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aF8aOfY7h2Ax"},"outputs":[],"source":["import scipy.special\n","from lightgbm.callback import early_stopping, log_evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwMzbny0tRq1"},"outputs":[],"source":["y_test.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NabO40MbK0oo"},"outputs":[],"source":["\n","# Config\n","n_splits = 10\n","random_state = 42\n","random_state_list =[42]\n","n_estimators = 200\n","device = 'cpu'\n","early_stopping_rounds = 444\n","verbose = False\n","\n","splitter = Splitter(n_splits=n_splits, cat_df= y_train)\n","splits = splitter.split_data(X_train, y_train, random_state_list=random_state_list)\n","\n","classifier = Classifier(n_estimators=n_estimators, device=device, random_state=random_state)\n","test_predss = np.zeros((test.shape[0]))\n","oof_predss = np.zeros((X_train.shape[0]))\n","\n","ensemble_score = []\n","weights = []\n","models_name = [_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)]\n","trained_models = dict(zip(models_name, [[] for _ in range(classifier.len_models)]))\n","score_dict = dict(zip(classifier.models_name, [[] for _ in range(len(classifier.models_name))]))\n","\n","for i, (X_train_, x_test, y_train_, y_test, val_index) in enumerate(splits):\n","    n = i % n_splits\n","    m = i // n_splits\n","\n","    classifier = Classifier(n_estimators, device, random_state)\n","    models = classifier.models\n","\n","    oof_preds = []\n","    test_preds = []\n","\n","    for name, model in models.items():\n","        if ('xgb' in name) or ('lgb' in name):\n","            model.fit(X_train_, y_train_, eval_set=[(x_test, y_test)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n","        else:\n","            model.fit(X_train_, y_train_)\n","\n","        if name in trained_models.keys():\n","            trained_models[f'{name}'].append(deepcopy(model))\n","\n","        test_pred = model.predict(test)\n","        y_val_pred = model.predict(x_test)\n","        score = accuracy_score(y_test, y_val_pred)\n","\n","        score_dict[name].append(score)\n","        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Accuracy score: {score:.5f}')\n","\n","        oof_preds.append(y_val_pred)\n","        test_preds.append(test_pred)\n","\n","    optweights = OptunaWeights(random_state, n_trials=100)\n","    y_val_pred = optweights.fit_predict(y_test.values.ravel(), np.array(oof_preds).T)\n","\n","    score = accuracy_score(y_test, y_val_pred)\n","    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] Accuracy score: {score:.5f} \\n')\n","    ensemble_score.append(score)\n","    weights.append(optweights.weights)\n","\n","    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n","    oof_predss[val_index] = optweights.predict(oof_preds)\n","\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGgpbdGyh_oB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_uxcoCeiI0T"},"outputs":[],"source":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","cell_execution_strategy":"setup","authorship_tag":"ABX9TyOufuLgiFSpNnrlt7B1PH8I"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}